{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PMI(w, l) = log ( P(w | l) / P(w) ) where P(w | l) is a probability that word w appears in a set of sentences of grade level l and P(w) is a probability of word w being within the entire training corpus. \n",
    "\n",
    "PPMI(w, l) = max(PMI(w, l), 0).\n",
    "\n",
    "Words\n",
    "with negative PMI scores have a negative correlation against l that means w tends to appear across different sentence levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    data = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            data.append(line.strip())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_vocab(y, moses=False):\n",
    "    vocab_counter=Counter()\n",
    "    for line in y:\n",
    "        vocab_counter.update(line.split(\" \"))\n",
    "    return vocab_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_files = \"\" # file mapping grade to articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_grade_data = {grade: [] for grade  in range(2, 11)}\n",
    "for grade in range(2, 11):\n",
    "    for filename in grade_files:\n",
    "        all_grade_data[grade].extend(read_file(filename))\n",
    "    \n",
    "grade_vocab = {}\n",
    "for grade in range(2, 11):\n",
    "    grade_vocab[grade] = get_vocab(all_grade_data[grade])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab = # vocab from all articles using get_vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "PMI = {}\n",
    "\n",
    "for word in all_vocab:\n",
    "    for grade in range(2, 11):\n",
    "        PMI[(word, grade)] = math.log( (complex_vocab[word] / all_vocab[word] ) / (all_vocab[word] / sum(all_vocab.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(PMI, open(\"data/PMI_nltk.pkl\",\"wb\"))\n",
    "pickle.dump(grade_vocab, open(\"data/GradeVocab_nltk.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "PMI = pickle.load(open(\"data/PMI_nltk.pkl\",\"rb\"))\n",
    "grade_vocab = pickle.load(open(\"data/GradeVocab_nltk.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positive_PMI(grade):\n",
    "    words = []\n",
    "    for word in grade_vocab[grade]:\n",
    "        if PMI[(word, grade)] > 0:\n",
    "            words.append(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = get_positive_PMI(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word=\"washington\"\n",
    "[PMI[(word, grade)] for grade in range(2,13) if (word, grade) in PMI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename, sep=\"~\"):\n",
    "    data = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            data.append(line.strip().split(sep))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = {'2': '<TWO>', '3': '<THREE>' , '4': '<FOUR>', '5': '<FIVE>', '6' : '<SIX>',\n",
    "     '7': '<SEVEN>', '8':'<EIGHT>', '9' : '<NINE>', '10': '<TEN>', '11': '<ELEVEN>', '12' : '<TWELVE>'}\n",
    "inv_map = {v: int(k) for k, v in token.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_src = read_file(\"../data/dev.src.nograde\")\n",
    "dev_tgt = read_file(\"../data/dev.tgt\")\n",
    "grades =  read_file(\"../data/dev.src-tgt.grade\", \"\\t\")\n",
    "oracle_const = read_file(\"../experiments/exp-1/data/test.const.src.oracle.del\", \"  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_grade = [inv_map[x[1]] for x in grades]\n",
    "source_grade = [inv_map[x[0]] for x in grades] \n",
    "src_text = [x[0].lower() for x in dev_src]\n",
    "tgt_text = [x[0].lower() for x in dev_tgt]\n",
    "oracle_bpes = [x[1].lower() for x in oracle_const]\n",
    "src_bpes = [x[0].lower() for x in oracle_const]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "flatten = lambda t: [item for sublist in t for item in sublist]\n",
    "\n",
    "all_entities = []\n",
    "all_entities_caps = []\n",
    "for i in tqdm(range(len(dev_src))):\n",
    "    entities = flatten([X.text.lower().split() for X in nlp(dev_src[i][0]).ents])\n",
    "    all_entities.append(entities)\n",
    "    all_entities_caps.append(flatten([X.text.split() for X in nlp(dev_src[i][0]).ents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/test_ents.pkl\",\"wb\") as f:\n",
    "    pickle.dump([all_entities, all_entities_caps], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/test_ents.pkl\",\"rb\") as f:\n",
    "    all_entities, all_entities_caps = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../readability/\")\n",
    "from compute_grade_stats import get_text_grade_score, clip_value\n",
    "from SARI import SARIsent\n",
    "import numpy as np\n",
    "clip_val=False\n",
    "import sacrebleu\n",
    "\n",
    "def get_sari(src, tgt, out):\n",
    "    return SARIsent(src.strip(), out.strip(), [tgt.strip()])[0]\n",
    "\n",
    "def get_bleu(tgt, out):\n",
    "    return sacrebleu.sentence_bleu(out, [tgt]).score\n",
    "\n",
    "def get_grade(text, clip_val=True, grade_type=\"ARI\"):\n",
    "    return get_text_grade_score(text, clip_val, grade_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "words = []\n",
    "pos_const = []\n",
    "neg_const = []\n",
    "overlap_score_pos = []\n",
    "overlap_score_neg = []\n",
    "include_ent = True\n",
    "include_stop_words = True\n",
    "include_punct=True\n",
    "neg_new_source = []\n",
    "count = 0\n",
    "for i in tqdm(range(len(src_text))):\n",
    "#     sg = source_grade[i]\n",
    "    tg = target_grade[i]\n",
    "    sg = int(get_grade(src_text[i], clip_val=True, grade_type=\"ARI\"))\n",
    "    pos_words = []\n",
    "    \n",
    "    tgt_words = word_tokenize(tgt_text[i])\n",
    "    src_words = word_tokenize(src_text[i])\n",
    "    \n",
    "    pos_oracle = [x for x in src_words if x in tgt_words]\n",
    "    \n",
    "    neg_words = []\n",
    "    for word in word_tokenize(src_text[i]):\n",
    "\n",
    "        if (word, sg) in PMI  and (word, tg) in PMI and PMI[(word, sg)] > 0 and PMI[(word, tg)] < 0:\n",
    "            neg_words.append(word)\n",
    "    \n",
    "    set_exclude = []\n",
    "    if include_ent:\n",
    "        set_exclude.extend(all_entities[i])\n",
    "    if include_stop_words:\n",
    "        set_exclude.extend(stop_words)\n",
    "    \n",
    "    neg_words = [x for x in neg_words if not (x in set_exclude)]\n",
    "    neg_const.append(neg_words)\n",
    "    \n",
    "    new_pos = [x for x in word_tokenize(src_text[i]) if x not in neg_words]\n",
    "    neg_new_source.append(new_pos)\n",
    "    if len(new_pos) == 0:\n",
    "        new_pos = src_words\n",
    "    prec_score = len(set(pos_oracle).intersection(set(new_pos))) / len(set(pos_oracle))\n",
    "    recall_score = len(set(pos_oracle).intersection(set(new_pos))) / len(set(new_pos))\n",
    "    overlap_score_neg.append((prec_score, recall_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prec_rec(scores):\n",
    "    prec = [x[0] for x in scores]\n",
    "    rec = [x[1] for x in scores]\n",
    "    print(\"Precision: \", sum(prec)/len(prec))\n",
    "    print(\"Recall: \", sum(rec)/len(rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_prec_rec(overlap_score_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"neg_const_test_ar.txt\",\"w\") as f:\n",
    "    for words in neg_new_source:\n",
    "        f.write((\" \").join(words) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, call bash  python $bpe_scripts_path/apply_bpe.py --codes ../experiments/exp-1/data/bpe < neg_const_dev_recall.txt > neg_const_dev_recall.bpe  to generate bpes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_const_bpe = []\n",
    "with open(\"neg_const_test_ar.bpe\") as f:\n",
    "    for line in f:\n",
    "        neg_const_bpe.append(line.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_score = []\n",
    "for i in range(len(neg_const_bpe)):\n",
    "    pos_oracle = oracle_bpes[i].split(\" \")\n",
    "    pos_words = [x for x in src_bpes[i].split(\" \")[1:] if x not in neg_const_bpe[i]]\n",
    "    prec_score = len(set(pos_oracle).intersection(set(pos_words))) / len(set(pos_oracle))\n",
    "    if len(set(pos_words)) > 0:\n",
    "        rec_score = len(set(pos_oracle).intersection(set(pos_words))) / len(set(pos_words))\n",
    "    else:\n",
    "        rec_score = 0\n",
    "    overlap_score.append((prec_score, rec_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = [x[0] for x in overlap_score ]\n",
    "rec = [x[1] for x in overlap_score ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(prec)/len(prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# includes more words than required\n",
    "sum(rec)/len(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
